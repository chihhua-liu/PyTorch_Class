{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "class CustomImageDataset(Dataset):\n",
    "  \n",
    "    def read_data_set(self):\n",
    "        all_img_files = []\n",
    "        all_labels = []\n",
    "        class_names = []\n",
    "        for index, i in enumerate(os.listdir(self.data_set_path)):\n",
    "            class_names.append(i)\n",
    "            for j in os.listdir(self.data_set_path+'/'+i):\n",
    "                all_img_files.append(self.data_set_path+'/'+i+'/'+j)\n",
    "                all_labels.append(index)\n",
    "        return all_img_files, all_labels, len(all_img_files), len(class_names)\n",
    "    def __init__(self, data_set_path, transforms=None):\n",
    "        self.data_set_path = data_set_path\n",
    "\n",
    "        self.image_files_path, self.labels, self.length, self.num_classes = self.read_data_set()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_files_path[index])\n",
    "        image = ImageOps.grayscale(image)\n",
    "        #image = image.convert(\"RGB\")\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        label = self.labels[index]\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = 'samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "image_size = 128 * 128\n",
    "transforms_train = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                       transforms.RandomRotation(10.),\n",
    "                                       transforms.ToTensor()])\n",
    "train_data_set = CustomImageDataset(data_set_path=\"./cat_dog/train\", transforms=transforms_train)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=train_data_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=128*128, h_dim=400, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim) \n",
    "        self.fc3 = nn.Linear(h_dim, z_dim) \n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return F.sigmoid(self.fc5(h))\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var\n",
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100], Step [2/4], Reconst Loss: 1595640.2500, KL Div: 24655.1172\n",
      "Epoch[1/100], Step [4/4], Reconst Loss: 193615.6094, KL Div: 2509.4373\n",
      "Epoch[2/100], Step [2/4], Reconst Loss: 1488045.7500, KL Div: 16328.0508\n",
      "Epoch[2/100], Step [4/4], Reconst Loss: 181206.9688, KL Div: 888.4000\n",
      "Epoch[3/100], Step [2/4], Reconst Loss: 1454680.8750, KL Div: 5395.0483\n",
      "Epoch[3/100], Step [4/4], Reconst Loss: 181468.6562, KL Div: 400.7295\n",
      "Epoch[4/100], Step [2/4], Reconst Loss: 1449862.5000, KL Div: 3034.8708\n",
      "Epoch[4/100], Step [4/4], Reconst Loss: 182156.5938, KL Div: 348.9175\n",
      "Epoch[5/100], Step [2/4], Reconst Loss: 1438007.0000, KL Div: 2440.5532\n",
      "Epoch[5/100], Step [4/4], Reconst Loss: 179315.8438, KL Div: 309.1459\n",
      "Epoch[6/100], Step [2/4], Reconst Loss: 1431224.6250, KL Div: 2595.8833\n",
      "Epoch[6/100], Step [4/4], Reconst Loss: 177986.1562, KL Div: 268.9095\n",
      "Epoch[7/100], Step [2/4], Reconst Loss: 1422469.2500, KL Div: 2527.5278\n",
      "Epoch[7/100], Step [4/4], Reconst Loss: 177802.5000, KL Div: 311.4706\n",
      "Epoch[8/100], Step [2/4], Reconst Loss: 1415880.0000, KL Div: 2589.7979\n",
      "Epoch[8/100], Step [4/4], Reconst Loss: 176531.3438, KL Div: 348.6731\n",
      "Epoch[9/100], Step [2/4], Reconst Loss: 1408430.0000, KL Div: 3282.4507\n",
      "Epoch[9/100], Step [4/4], Reconst Loss: 178404.3438, KL Div: 467.2301\n",
      "Epoch[10/100], Step [2/4], Reconst Loss: 1392471.0000, KL Div: 3377.7275\n",
      "Epoch[10/100], Step [4/4], Reconst Loss: 173460.4688, KL Div: 364.0054\n",
      "Epoch[11/100], Step [2/4], Reconst Loss: 1387122.6250, KL Div: 2872.0085\n",
      "Epoch[11/100], Step [4/4], Reconst Loss: 172927.4062, KL Div: 279.4611\n",
      "Epoch[12/100], Step [2/4], Reconst Loss: 1383170.5000, KL Div: 2427.3174\n",
      "Epoch[12/100], Step [4/4], Reconst Loss: 171794.0469, KL Div: 263.1650\n",
      "Epoch[13/100], Step [2/4], Reconst Loss: 1379829.0000, KL Div: 2551.1421\n",
      "Epoch[13/100], Step [4/4], Reconst Loss: 168915.7969, KL Div: 436.9928\n",
      "Epoch[14/100], Step [2/4], Reconst Loss: 1389268.0000, KL Div: 3115.7217\n",
      "Epoch[14/100], Step [4/4], Reconst Loss: 170290.7500, KL Div: 417.9495\n",
      "Epoch[15/100], Step [2/4], Reconst Loss: 1382850.2500, KL Div: 3345.2930\n",
      "Epoch[15/100], Step [4/4], Reconst Loss: 167124.8594, KL Div: 524.8548\n",
      "Epoch[16/100], Step [2/4], Reconst Loss: 1368364.2500, KL Div: 3910.8875\n",
      "Epoch[16/100], Step [4/4], Reconst Loss: 169954.8750, KL Div: 502.7936\n",
      "Epoch[17/100], Step [2/4], Reconst Loss: 1363564.2500, KL Div: 3680.6733\n",
      "Epoch[17/100], Step [4/4], Reconst Loss: 166045.9219, KL Div: 508.0202\n",
      "Epoch[18/100], Step [2/4], Reconst Loss: 1364168.0000, KL Div: 3777.8804\n",
      "Epoch[18/100], Step [4/4], Reconst Loss: 173568.8750, KL Div: 476.2397\n",
      "Epoch[19/100], Step [2/4], Reconst Loss: 1348833.5000, KL Div: 4223.6675\n",
      "Epoch[19/100], Step [4/4], Reconst Loss: 167906.7344, KL Div: 553.1500\n",
      "Epoch[20/100], Step [2/4], Reconst Loss: 1354528.7500, KL Div: 4048.2761\n",
      "Epoch[20/100], Step [4/4], Reconst Loss: 171938.9688, KL Div: 529.2356\n",
      "Epoch[21/100], Step [2/4], Reconst Loss: 1356010.3750, KL Div: 4263.4263\n",
      "Epoch[21/100], Step [4/4], Reconst Loss: 174258.4375, KL Div: 451.2250\n",
      "Epoch[22/100], Step [2/4], Reconst Loss: 1353589.6250, KL Div: 4506.1357\n",
      "Epoch[22/100], Step [4/4], Reconst Loss: 166526.7344, KL Div: 476.5838\n",
      "Epoch[23/100], Step [2/4], Reconst Loss: 1360969.0000, KL Div: 4030.3643\n",
      "Epoch[23/100], Step [4/4], Reconst Loss: 166242.4531, KL Div: 647.6024\n",
      "Epoch[24/100], Step [2/4], Reconst Loss: 1353349.3750, KL Div: 3931.7759\n",
      "Epoch[24/100], Step [4/4], Reconst Loss: 169333.6250, KL Div: 593.6508\n",
      "Epoch[25/100], Step [2/4], Reconst Loss: 1339742.8750, KL Div: 4650.9194\n",
      "Epoch[25/100], Step [4/4], Reconst Loss: 168777.2500, KL Div: 534.1125\n",
      "Epoch[26/100], Step [2/4], Reconst Loss: 1337716.0000, KL Div: 4951.4141\n",
      "Epoch[26/100], Step [4/4], Reconst Loss: 167168.2969, KL Div: 703.1341\n",
      "Epoch[27/100], Step [2/4], Reconst Loss: 1354346.3750, KL Div: 4914.5049\n",
      "Epoch[27/100], Step [4/4], Reconst Loss: 165367.8906, KL Div: 716.5023\n",
      "Epoch[28/100], Step [2/4], Reconst Loss: 1329147.5000, KL Div: 5994.2705\n",
      "Epoch[28/100], Step [4/4], Reconst Loss: 163413.5938, KL Div: 618.1099\n",
      "Epoch[29/100], Step [2/4], Reconst Loss: 1331667.7500, KL Div: 5160.8047\n",
      "Epoch[29/100], Step [4/4], Reconst Loss: 162466.1875, KL Div: 841.6453\n",
      "Epoch[30/100], Step [2/4], Reconst Loss: 1322084.0000, KL Div: 5563.3467\n",
      "Epoch[30/100], Step [4/4], Reconst Loss: 163680.1875, KL Div: 595.2565\n",
      "Epoch[31/100], Step [2/4], Reconst Loss: 1310836.6250, KL Div: 5919.5322\n",
      "Epoch[31/100], Step [4/4], Reconst Loss: 169296.0625, KL Div: 600.9830\n",
      "Epoch[32/100], Step [2/4], Reconst Loss: 1315363.3750, KL Div: 5396.3535\n",
      "Epoch[32/100], Step [4/4], Reconst Loss: 165177.3750, KL Div: 599.1315\n",
      "Epoch[33/100], Step [2/4], Reconst Loss: 1308258.3750, KL Div: 5307.8730\n",
      "Epoch[33/100], Step [4/4], Reconst Loss: 160760.1562, KL Div: 759.8005\n",
      "Epoch[34/100], Step [2/4], Reconst Loss: 1322631.1250, KL Div: 5415.3062\n",
      "Epoch[34/100], Step [4/4], Reconst Loss: 168451.1719, KL Div: 716.3978\n",
      "Epoch[35/100], Step [2/4], Reconst Loss: 1314587.1250, KL Div: 5812.1328\n",
      "Epoch[35/100], Step [4/4], Reconst Loss: 162962.3750, KL Div: 676.4684\n",
      "Epoch[36/100], Step [2/4], Reconst Loss: 1303972.8750, KL Div: 6068.3086\n",
      "Epoch[36/100], Step [4/4], Reconst Loss: 165320.3906, KL Div: 680.7373\n",
      "Epoch[37/100], Step [2/4], Reconst Loss: 1306295.6250, KL Div: 5956.9053\n",
      "Epoch[37/100], Step [4/4], Reconst Loss: 159114.3125, KL Div: 817.0459\n",
      "Epoch[38/100], Step [2/4], Reconst Loss: 1309710.0000, KL Div: 6040.4600\n",
      "Epoch[38/100], Step [4/4], Reconst Loss: 165895.7031, KL Div: 729.3168\n",
      "Epoch[39/100], Step [2/4], Reconst Loss: 1315673.6250, KL Div: 5775.5361\n",
      "Epoch[39/100], Step [4/4], Reconst Loss: 167212.0625, KL Div: 669.2018\n",
      "Epoch[40/100], Step [2/4], Reconst Loss: 1307433.7500, KL Div: 7508.6050\n",
      "Epoch[40/100], Step [4/4], Reconst Loss: 165261.6562, KL Div: 703.7479\n",
      "Epoch[41/100], Step [2/4], Reconst Loss: 1308239.5000, KL Div: 6503.5317\n",
      "Epoch[41/100], Step [4/4], Reconst Loss: 162416.2656, KL Div: 626.8484\n",
      "Epoch[42/100], Step [2/4], Reconst Loss: 1291271.0000, KL Div: 6002.3101\n",
      "Epoch[42/100], Step [4/4], Reconst Loss: 160849.2969, KL Div: 837.9982\n",
      "Epoch[43/100], Step [2/4], Reconst Loss: 1301488.5000, KL Div: 6014.7070\n",
      "Epoch[43/100], Step [4/4], Reconst Loss: 162159.6250, KL Div: 717.4511\n",
      "Epoch[44/100], Step [2/4], Reconst Loss: 1303373.2500, KL Div: 5738.4150\n",
      "Epoch[44/100], Step [4/4], Reconst Loss: 158940.8281, KL Div: 772.1683\n",
      "Epoch[45/100], Step [2/4], Reconst Loss: 1296753.5000, KL Div: 6012.7461\n",
      "Epoch[45/100], Step [4/4], Reconst Loss: 161578.1562, KL Div: 880.4561\n",
      "Epoch[46/100], Step [2/4], Reconst Loss: 1287216.2500, KL Div: 6406.0435\n",
      "Epoch[46/100], Step [4/4], Reconst Loss: 167401.1875, KL Div: 655.8913\n",
      "Epoch[47/100], Step [2/4], Reconst Loss: 1274086.8750, KL Div: 6944.6162\n",
      "Epoch[47/100], Step [4/4], Reconst Loss: 164286.5625, KL Div: 773.6795\n",
      "Epoch[48/100], Step [2/4], Reconst Loss: 1304761.2500, KL Div: 6436.8120\n",
      "Epoch[48/100], Step [4/4], Reconst Loss: 158098.6719, KL Div: 826.6309\n",
      "Epoch[49/100], Step [2/4], Reconst Loss: 1285564.0000, KL Div: 6725.8076\n",
      "Epoch[49/100], Step [4/4], Reconst Loss: 159884.6719, KL Div: 730.8741\n",
      "Epoch[50/100], Step [2/4], Reconst Loss: 1306869.5000, KL Div: 6314.1089\n",
      "Epoch[50/100], Step [4/4], Reconst Loss: 159698.9219, KL Div: 839.6473\n",
      "Epoch[51/100], Step [2/4], Reconst Loss: 1291964.0000, KL Div: 6452.2993\n",
      "Epoch[51/100], Step [4/4], Reconst Loss: 162185.2969, KL Div: 643.5143\n",
      "Epoch[52/100], Step [2/4], Reconst Loss: 1275660.8750, KL Div: 6764.6440\n",
      "Epoch[52/100], Step [4/4], Reconst Loss: 160586.1562, KL Div: 745.0201\n",
      "Epoch[53/100], Step [2/4], Reconst Loss: 1291985.6250, KL Div: 6465.3169\n",
      "Epoch[53/100], Step [4/4], Reconst Loss: 162081.0469, KL Div: 799.7916\n",
      "Epoch[54/100], Step [2/4], Reconst Loss: 1280309.7500, KL Div: 6333.2754\n",
      "Epoch[54/100], Step [4/4], Reconst Loss: 162533.2031, KL Div: 722.2950\n",
      "Epoch[55/100], Step [2/4], Reconst Loss: 1268722.3750, KL Div: 6582.8267\n",
      "Epoch[55/100], Step [4/4], Reconst Loss: 159376.0469, KL Div: 814.1257\n",
      "Epoch[56/100], Step [2/4], Reconst Loss: 1272139.7500, KL Div: 6595.4951\n",
      "Epoch[56/100], Step [4/4], Reconst Loss: 156778.6094, KL Div: 713.8799\n",
      "Epoch[57/100], Step [2/4], Reconst Loss: 1270132.5000, KL Div: 7140.4067\n",
      "Epoch[57/100], Step [4/4], Reconst Loss: 160951.0781, KL Div: 835.8796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[58/100], Step [2/4], Reconst Loss: 1268190.6250, KL Div: 6880.3403\n",
      "Epoch[58/100], Step [4/4], Reconst Loss: 164702.5469, KL Div: 758.7197\n",
      "Epoch[59/100], Step [2/4], Reconst Loss: 1261343.5000, KL Div: 6977.5488\n",
      "Epoch[59/100], Step [4/4], Reconst Loss: 160610.0312, KL Div: 814.8655\n",
      "Epoch[60/100], Step [2/4], Reconst Loss: 1273879.5000, KL Div: 6407.4932\n",
      "Epoch[60/100], Step [4/4], Reconst Loss: 154767.5312, KL Div: 708.5917\n",
      "Epoch[61/100], Step [2/4], Reconst Loss: 1258059.1250, KL Div: 6618.1660\n",
      "Epoch[61/100], Step [4/4], Reconst Loss: 155274.2500, KL Div: 1036.7024\n",
      "Epoch[62/100], Step [2/4], Reconst Loss: 1269985.2500, KL Div: 6859.4365\n",
      "Epoch[62/100], Step [4/4], Reconst Loss: 157404.5156, KL Div: 1034.2200\n",
      "Epoch[63/100], Step [2/4], Reconst Loss: 1267698.5000, KL Div: 6655.5928\n",
      "Epoch[63/100], Step [4/4], Reconst Loss: 154888.9688, KL Div: 901.1783\n",
      "Epoch[64/100], Step [2/4], Reconst Loss: 1270701.7500, KL Div: 7132.3267\n",
      "Epoch[64/100], Step [4/4], Reconst Loss: 158239.1094, KL Div: 919.2543\n",
      "Epoch[65/100], Step [2/4], Reconst Loss: 1283134.8750, KL Div: 6979.4185\n",
      "Epoch[65/100], Step [4/4], Reconst Loss: 157456.4375, KL Div: 720.3128\n",
      "Epoch[66/100], Step [2/4], Reconst Loss: 1270846.7500, KL Div: 5802.0508\n",
      "Epoch[66/100], Step [4/4], Reconst Loss: 161525.1875, KL Div: 766.8915\n",
      "Epoch[67/100], Step [2/4], Reconst Loss: 1261930.2500, KL Div: 6465.3691\n",
      "Epoch[67/100], Step [4/4], Reconst Loss: 155974.9531, KL Div: 584.4869\n",
      "Epoch[68/100], Step [2/4], Reconst Loss: 1275260.2500, KL Div: 6298.7222\n",
      "Epoch[68/100], Step [4/4], Reconst Loss: 156416.1562, KL Div: 829.6238\n",
      "Epoch[69/100], Step [2/4], Reconst Loss: 1275715.7500, KL Div: 7853.0576\n",
      "Epoch[69/100], Step [4/4], Reconst Loss: 160034.4219, KL Div: 846.0621\n",
      "Epoch[70/100], Step [2/4], Reconst Loss: 1261405.5000, KL Div: 7282.3760\n",
      "Epoch[70/100], Step [4/4], Reconst Loss: 159592.1719, KL Div: 937.5504\n",
      "Epoch[71/100], Step [2/4], Reconst Loss: 1269488.0000, KL Div: 7048.2471\n",
      "Epoch[71/100], Step [4/4], Reconst Loss: 153160.4688, KL Div: 1040.2097\n",
      "Epoch[72/100], Step [2/4], Reconst Loss: 1272501.5000, KL Div: 7394.5342\n",
      "Epoch[72/100], Step [4/4], Reconst Loss: 157024.3438, KL Div: 757.1301\n",
      "Epoch[73/100], Step [2/4], Reconst Loss: 1253053.5000, KL Div: 7219.4023\n",
      "Epoch[73/100], Step [4/4], Reconst Loss: 157080.0000, KL Div: 857.8295\n",
      "Epoch[74/100], Step [2/4], Reconst Loss: 1273896.8750, KL Div: 7165.3145\n",
      "Epoch[74/100], Step [4/4], Reconst Loss: 155458.6719, KL Div: 860.5037\n",
      "Epoch[75/100], Step [2/4], Reconst Loss: 1260497.3750, KL Div: 7105.4062\n",
      "Epoch[75/100], Step [4/4], Reconst Loss: 155364.5469, KL Div: 1002.7262\n",
      "Epoch[76/100], Step [2/4], Reconst Loss: 1245902.1250, KL Div: 7188.3887\n",
      "Epoch[76/100], Step [4/4], Reconst Loss: 153937.9844, KL Div: 936.0803\n",
      "Epoch[77/100], Step [2/4], Reconst Loss: 1249244.1250, KL Div: 7295.9414\n",
      "Epoch[77/100], Step [4/4], Reconst Loss: 158419.1094, KL Div: 823.5255\n",
      "Epoch[78/100], Step [2/4], Reconst Loss: 1244899.6250, KL Div: 7435.8032\n",
      "Epoch[78/100], Step [4/4], Reconst Loss: 158109.7500, KL Div: 851.5585\n",
      "Epoch[79/100], Step [2/4], Reconst Loss: 1256269.7500, KL Div: 6867.7061\n",
      "Epoch[79/100], Step [4/4], Reconst Loss: 161115.8281, KL Div: 765.8929\n",
      "Epoch[80/100], Step [2/4], Reconst Loss: 1256469.0000, KL Div: 7300.6675\n",
      "Epoch[80/100], Step [4/4], Reconst Loss: 159521.1250, KL Div: 965.8555\n",
      "Epoch[81/100], Step [2/4], Reconst Loss: 1252252.0000, KL Div: 7394.5103\n",
      "Epoch[81/100], Step [4/4], Reconst Loss: 157724.0469, KL Div: 893.7013\n",
      "Epoch[82/100], Step [2/4], Reconst Loss: 1263154.2500, KL Div: 7196.1826\n",
      "Epoch[82/100], Step [4/4], Reconst Loss: 154135.5469, KL Div: 931.6793\n",
      "Epoch[83/100], Step [2/4], Reconst Loss: 1237780.2500, KL Div: 7215.9834\n",
      "Epoch[83/100], Step [4/4], Reconst Loss: 159767.3594, KL Div: 901.0062\n",
      "Epoch[84/100], Step [2/4], Reconst Loss: 1254683.5000, KL Div: 7332.4326\n",
      "Epoch[84/100], Step [4/4], Reconst Loss: 160711.8906, KL Div: 925.5405\n",
      "Epoch[85/100], Step [2/4], Reconst Loss: 1247589.1250, KL Div: 7572.1274\n",
      "Epoch[85/100], Step [4/4], Reconst Loss: 154908.9062, KL Div: 769.4476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        x = x.to(device).view(-1, image_size)\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
    "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 2 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(batch_size, z_dim).to(device)\n",
    "        out = model.decode(z).view(-1, 1, 128, 128)\n",
    "        save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "        out, _, _ = model(x)\n",
    "        x_concat = torch.cat([x.view(-1, 1, 128, 128), out.view(-1, 1, 128, 128)], dim=3)\n",
    "        save_image(x_concat, os.path.join(sample_dir, 'reconst-{}.png'.format(epoch+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg \n",
    "import numpy as np\n",
    "reconsPath = './samples/reconst-15.png'\n",
    "Image = mpimg.imread(reconsPath)\n",
    "plt.imshow(Image)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genPath = './samples/sampled-15.png'\n",
    "Image = mpimg.imread(genPath)\n",
    "plt.imshow(Image) \n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
