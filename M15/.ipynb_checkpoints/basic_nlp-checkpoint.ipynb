{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 15: NLP & Word2vec :  Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"新的數學方法和概念，常常比解決數學問題本身更重要。\",\n",
    "    \"在數學中，我們發現真理的主要工具是歸納和模擬。\",\n",
    "    \"數學方法滲透並支配著一切自然科學的理論分支。它愈來愈成為衡量科學成就的主要標志了。\",\n",
    "    \"第一是數學，第二是數學，第三是數學。\",\n",
    "    \"歷史使人賢明，詩造成氣質高雅的人，數學使人高尚，自然哲學使人深沈，道德使人穩重，而倫理學和修辭學則使人善於爭論。\"\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.647 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['新', '的', '數學', '方法', '和', '概念', '，', '常常', '比解', '決數學', '問題', '本身', '更', '重要', '。'], ['在', '數學', '中', '，', '我們', '發現', '真理', '的', '主要', '工具', '是', '歸納', '和', '模擬', '。'], ['數學', '方法', '滲透並', '支配', '著', '一切', '自然科', '學', '的', '理論', '分支', '。', '它', '愈來', '愈成', '為', '衡量', '科學', '成就', '的', '主要', '標志', '了', '。'], ['第一', '是', '數學', '，', '第二', '是', '數學', '，', '第三', '是', '數學', '。'], ['歷史', '使', '人賢明', '，', '詩', '造成', '氣質', '高雅', '的', '人', '，', '數學', '使', '人', '高尚', '，', '自然', '哲學', '使', '人', '深沈', '，', '道德', '使人', '穩重', '，', '而倫理學', '和', '修辭', '學則', '使', '人善', '於', '爭論', '。']]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "words_list = [list(jieba.cut(doc)) for doc in docs]\n",
    "print(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'自然科', '哲學', '了', '道德', '而倫理學', '愈來', '主要', '第三', '學則', '高雅', '於', '新', '數學', '決數學', '為', '穩重', '，', '概念', '更', '第二', '它', '理論', '歷史', '詩', '自然', '深沈', '真理', '修辭', '和', '我們', '本身', '發現', '支配', '工具', '人賢明', '高尚', '爭論', '在', '模擬', '一切', '方法', '衡量', '科學', '使人', '。', '的', '問題', '歸納', '分支', '著', '滲透並', '學', '愈成', '人善', '造成', '中', '比解', '重要', '是', '標志', '氣質', '常常', '成就', '第一', '使', '人'}\n"
     ]
    }
   ],
   "source": [
    "# build word dictionary\n",
    "vocb = set([word for words in words_list for word in words])\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}\n",
    "print(vocb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'自然科': 0, '哲學': 1, '了': 2, '道德': 3, '而倫理學': 4, '愈來': 5, '主要': 6, '第三': 7, '學則': 8, '高雅': 9, '於': 10, '新': 11, '數學': 12, '決數學': 13, '為': 14, '穩重': 15, '，': 16, '概念': 17, '更': 18, '第二': 19, '它': 20, '理論': 21, '歷史': 22, '詩': 23, '自然': 24, '深沈': 25, '真理': 26, '修辭': 27, '和': 28, '我們': 29, '本身': 30, '發現': 31, '支配': 32, '工具': 33, '人賢明': 34, '高尚': 35, '爭論': 36, '在': 37, '模擬': 38, '一切': 39, '方法': 40, '衡量': 41, '科學': 42, '使人': 43, '。': 44, '的': 45, '問題': 46, '歸納': 47, '分支': 48, '著': 49, '滲透並': 50, '學': 51, '愈成': 52, '人善': 53, '造成': 54, '中': 55, '比解': 56, '重要': 57, '是': 58, '標志': 59, '氣質': 60, '常常': 61, '成就': 62, '第一': 63, '使': 64, '人': 65}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '自然科', 1: '哲學', 2: '了', 3: '道德', 4: '而倫理學', 5: '愈來', 6: '主要', 7: '第三', 8: '學則', 9: '高雅', 10: '於', 11: '新', 12: '數學', 13: '決數學', 14: '為', 15: '穩重', 16: '，', 17: '概念', 18: '更', 19: '第二', 20: '它', 21: '理論', 22: '歷史', 23: '詩', 24: '自然', 25: '深沈', 26: '真理', 27: '修辭', 28: '和', 29: '我們', 30: '本身', 31: '發現', 32: '支配', 33: '工具', 34: '人賢明', 35: '高尚', 36: '爭論', 37: '在', 38: '模擬', 39: '一切', 40: '方法', 41: '衡量', 42: '科學', 43: '使人', 44: '。', 45: '的', 46: '問題', 47: '歸納', 48: '分支', 49: '著', 50: '滲透並', 51: '學', 52: '愈成', 53: '人善', 54: '造成', 55: '中', 56: '比解', 57: '重要', 58: '是', 59: '標志', 60: '氣質', 61: '常常', 62: '成就', 63: '第一', 64: '使', 65: '人'}\n"
     ]
    }
   ],
   "source": [
    "print(idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(66, 200)\n"
     ]
    }
   ],
   "source": [
    "vocb_size = len(vocb)\n",
    "embedding_size = 200\n",
    "embeds = nn.Embedding(vocb_size, embedding_size) \n",
    "print(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx[\"數學\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n",
      "tensor([[ 2.0963,  0.1981, -1.7784,  1.0103, -2.1503, -0.3864, -1.3740, -0.8644,\n",
      "         -1.2454,  0.8026,  0.1935, -0.6487,  0.7849, -1.1615, -1.2914, -0.3952,\n",
      "         -0.8804,  1.3720,  0.9693, -0.3596,  0.3632, -0.8033, -1.7070, -0.3065,\n",
      "          0.1396,  0.8496,  0.8920, -0.8671, -0.9784,  2.3705,  1.9180,  0.5076,\n",
      "         -0.4949,  1.0286, -0.7519,  0.1273, -0.6449, -0.4252,  0.0296,  0.6249,\n",
      "         -0.5070,  2.3412,  0.3954, -0.8889, -1.1631, -0.3813, -0.6472, -0.1931,\n",
      "         -1.7338,  0.5116, -0.5269,  0.1209, -0.4763,  1.0263,  0.0412,  0.8848,\n",
      "          0.2248, -0.4626, -2.9310,  0.3479,  0.5311,  2.1211, -0.5958, -0.2230,\n",
      "         -1.6773,  0.2350,  0.2947, -0.3127,  0.3329,  0.4304,  1.0236, -0.5136,\n",
      "         -0.4097,  0.2150,  0.4288,  0.5572,  2.0048,  0.0365, -1.8211, -0.4501,\n",
      "         -0.8413, -1.0694, -0.1263,  2.0551,  0.2505,  1.5999,  0.1556, -0.5356,\n",
      "         -1.1692,  0.2557, -0.3139,  0.5163,  0.1416,  1.6194, -0.5456, -1.1551,\n",
      "         -0.7815,  0.7291, -0.3718, -0.0929, -0.4783,  0.1075,  0.2650, -0.2019,\n",
      "         -0.5395, -1.4294, -0.2067,  2.2225, -0.1624, -1.3340,  1.2521, -0.8404,\n",
      "          1.9127, -0.8947, -1.5471,  0.7581,  0.8065,  0.1048, -0.8639,  1.2451,\n",
      "         -0.9911,  0.8357,  0.4628,  0.6110, -0.7248, -0.4147, -0.6124,  0.7309,\n",
      "          0.5638, -1.5640,  0.1129,  0.3765, -0.7369, -1.0864, -2.3199,  2.2803,\n",
      "         -0.1125,  0.8290,  0.9053, -0.1357,  0.2914,  1.1742,  0.5168,  1.8718,\n",
      "         -0.1659,  1.7084,  0.9900,  0.4722, -0.5742, -1.9269,  0.2318,  0.2781,\n",
      "         -2.1145,  0.1385,  0.5233,  1.2951, -0.4185, -0.6262,  0.2626,  1.1274,\n",
      "          0.2387, -1.4193,  0.0586, -2.0579, -0.2837,  0.1290,  0.1335, -1.5203,\n",
      "         -0.3191,  0.1715, -0.9634,  1.2256,  1.0954, -0.8718, -1.2236,  1.4452,\n",
      "         -0.7214,  1.0959,  1.0903, -1.1195,  0.6221,  0.7947, -0.3920,  0.7949,\n",
      "         -0.9127, -0.3044,  0.1377, -1.0172,  0.8618, -1.6322, -0.8274, -0.4436,\n",
      "          1.2791, -0.2649, -1.1029,  1.3697, -1.2391,  0.2838,  0.5268,  2.1749]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "look_up = torch.LongTensor([word_to_idx[\"數學\"]])\n",
    "print(embeds(look_up).shape)\n",
    "print(embeds(look_up))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pre-trained word2vect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# download the model and return as object ready for use\n",
    "word2vec_model = api.load(\"glove-twitter-25\")\n",
    "#word2vec_model = gensim.models.KeyedVectors.load(\"news.word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193514, 25)\n"
     ]
    }
   ],
   "source": [
    "embedding_shape = word2vec_model.vectors.shape\n",
    "print(embedding_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "#initialized embedding layer\n",
    "embed = nn.Embedding(embedding_shape[0], embedding_shape[1])\n",
    "\n",
    "#copy pre-trained word2vect model\n",
    "embed.weight.data.copy_(torch.from_numpy(word2vec_model.vectors))\n",
    "\n",
    "#freeze weight\n",
    "embed.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25])\n",
      "tensor([[-1.2304,  0.4831,  0.1410, -0.0295, -0.6525, -0.1855,  2.1033,  1.7516,\n",
      "         -1.3001, -0.3211, -0.8477,  0.4200, -3.8823,  0.1964, -0.7286, -0.8527,\n",
      "          0.2317, -1.0763, -0.8302,  0.1081, -0.5102,  0.2769, -1.1895,  0.9809,\n",
      "         -0.1396]])\n"
     ]
    }
   ],
   "source": [
    "index = word2vec_model.key_to_index[\"happy\"]\n",
    "#查表\n",
    "print(embed(torch.LongTensor([index])).shape)\n",
    "print(embed(torch.LongTensor([index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20555 ,  0.14291 ,  0.72309 , -0.82082 , -0.48214 ,  0.36303 ,\n",
       "       -0.348   ,  2.2456  , -1.0749  , -0.79381 ,  0.33621 , -2.4247  ,\n",
       "       -0.44681 , -0.70426 ,  1.3172  ,  0.48233 ,  0.074172, -0.15488 ,\n",
       "        0.40502 , -0.29675 ,  0.89252 , -2.0936  ,  0.75108 ,  0.44415 ,\n",
       "        0.83416 ], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.vectors[5160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 225, 117, 635], [6111, 656]]\n"
     ]
    }
   ],
   "source": [
    "docs = [[\"i\",\"am\",\"good\",\"guy\"],[\"delicious\",\"food\"]]\n",
    "\n",
    "for doc in docs:\n",
    "    for word in doc:\n",
    "        word2vec_model.key_to_index[word]\n",
    "indexes = [[word2vec_model.key_to_index[word] for word in doc] for doc in docs]\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([10, 225, 117, 635]) list([6111, 656])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_indexes = (np.array(indexes))\n",
    "print(np_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
