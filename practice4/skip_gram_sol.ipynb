{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 4  Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "dtype = torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def read_data(filename):\n",
    "    with open(filename) as f:\n",
    "        data = []\n",
    "        for i in f.readlines():\n",
    "            data = data + i.strip().split(' ') # strip()\n",
    "    return data\n",
    "word_sequence = read_data('./toy_text.txt')\n",
    "# read data set\n",
    "print('Data size %d' % len(word_sequence))\n",
    "print(word_sequence[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate vocab\n",
    "vocab = list(set(word_sequence))  \n",
    "vocab.sort()\n",
    "print(vocab)\n",
    "# enumerate() 函式來同時輸出索引與元素 :　for index, value in enumerate(List):\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}  \n",
    "idx2word = {v: k for k, v in word2idx.items()} #items()函数以列表返回可遍历的(键, 值) 元组数组。　\n",
    "print('word2idx=\\n',word2idx)\n",
    "print('idx2word=\\n',idx2word)\n",
    "\n",
    "batch_size = 32\n",
    "embedding_size = 2  \n",
    "C = 2  \n",
    "voc_size = len(vocab)\n",
    "skip_grams = []\n",
    "# generate skip_gram\n",
    "for idx in range(C, len(word_sequence) - C): \n",
    "    center = word2idx[word_sequence[idx]]  # 中心词\n",
    "    if idx == 2:\n",
    "        print('center=',center)\n",
    "    # a = [1,2,3], b=[4,5,6] -> a+b = [1,2,3,4,5,6]\n",
    "    context_idx = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1)) \n",
    "    context = [word2idx[word_sequence[i]] for i in context_idx]\n",
    "    for w in context:\n",
    "        skip_grams.append([center, w])  \n",
    "print(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(skip_grams):\n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    for i in range(len(skip_grams)):\n",
    "        input_data.append(np.eye(voc_size)[skip_grams[i][0]]) # one hot 編碼for center as x(feature)\n",
    "        output_data.append(skip_grams[i][1]) # 前後字 = y (label=answer)\n",
    "    return input_data, output_data\n",
    "input_data, output_data = make_data(skip_grams)\n",
    "input_data, output_data = torch.Tensor(input_data), torch.LongTensor(output_data)\n",
    "print(input_data[:5])\n",
    "print(output_data[:5])\n",
    "\n",
    "dataset = Data.TensorDataset(input_data, output_data)\n",
    "loader = Data.DataLoader(dataset, batch_size, True) # True is shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        # finish this part\n",
    "        self.W = nn.Parameter(torch.randn(voc_size, embedding_size).type((dtype)))\n",
    "        self.V = nn.Parameter(torch.randn(embedding_size, voc_size).type((dtype)))\n",
    "    def forward(self, X):\n",
    "        # finish this part\n",
    "        hidden_layer = torch.matmul(X, self.W)  # hidden_layer : [batch_size, embedding_size]\n",
    "        output_layer = torch.matmul(hidden_layer, self.V)  # output_layer : [batch_size, voc_size]\n",
    "        return output_layer\n",
    "model = Word2Vec().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in range(2000):\n",
    "    for i, (batch_x, batch_y) in enumerate(loader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        pred = model(batch_x)\n",
    "        loss = criterion(pred, batch_y)\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(epoch + 1, i, loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw figure\n",
    "for i, label in enumerate(vocab):\n",
    "    W, WT = model.parameters()\n",
    "    x, y = float(W[i][0]), float(W[i][1])\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
